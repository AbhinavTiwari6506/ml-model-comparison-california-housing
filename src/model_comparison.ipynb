# 0) Install packages if not present
import sys, subprocess, pkgutil
def install_if_missing(pkg):
    if pkgutil.find_loader(pkg) is None:
        print(f"Installing {pkg} ...")
        subprocess.check_call([sys.executable, "-m", "pip", "install", pkg])
install_if_missing("xgboost")
install_if_missing("seaborn")

# 1) Imports
import os, io, json, warnings
warnings.filterwarnings("ignore")
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, KFold, cross_val_score
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
import xgboost as xgb

# Make plots appear inline
%matplotlib inline
sns.set(style="whitegrid")

# 2) Load data: prefer uploaded 'data.csv' otherwise use sklearn example
from google.colab import files
uploaded_files = list(files.upload().keys())  # opens upload dialog; OK to skip if using fallback

if 'data.csv' in uploaded_files:
    print("Using uploaded data.csv")
    df = pd.read_csv('data.csv')
    # Try to infer target: if a 'target' column exists use it; otherwise ask user to set target variable
    if 'target' in df.columns:
        target = 'target'
    else:
        # fallback: last numeric column
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        target = numeric_cols[-1]
        print(f"No 'target' column found — using inferred target: {target}")
else:
    print("No data.csv uploaded — using sklearn California housing dataset as fallback.")
    from sklearn.datasets import fetch_california_housing
    data = fetch_california_housing(as_frame=True)
    df = data.frame
    df = df.rename(columns={'MedHouseVal':'target'})
    target = 'target'

print("Data shape:", df.shape)
print("Columns:", df.columns.tolist()[:10], "..." if len(df.columns)>10 else "")

# 3) Basic preprocessing
# Fill missing values
df = df.copy()
df = df.fillna(method='ffill').fillna(method='bfill').fillna(0)

# If datetime-like column present, extract time features (optional)
datetime_cols = [c for c in df.columns if 'date' in c.lower() or 'time' in c.lower() or 'datetime' in c.lower()]
if datetime_cols:
    dtc = datetime_cols[0]
    df[dtc] = pd.to_datetime(df[dtc], errors='coerce')
    df['hour'] = df[dtc].dt.hour
    df['dayofweek'] = df[dtc].dt.dayofweek
    df['month'] = df[dtc].dt.month
    print(f"Extracted time features from {dtc}")

# Optional simple lag features if target exists
if target in df.columns:
    df['lag1_target'] = df[target].shift(1).fillna(method='bfill')
    df['rolling_mean_3'] = df[target].rolling(window=3, min_periods=1).mean().fillna(method='bfill')

# Select numeric features only
numeric_df = df.select_dtypes(include=[np.number]).copy()
if target not in numeric_df.columns:
    raise ValueError(f"Target column '{target}' not present as numeric after preprocessing. Please set correct target.")

# Drop constant cols
const_cols = numeric_df.columns[numeric_df.nunique() <= 1].tolist()
if const_cols:
    numeric_df = numeric_df.drop(columns=const_cols)

# 4) Feature/target split
X = numeric_df.drop(columns=[target])
y = numeric_df[target]

print("Feature count:", X.shape[1])

# 5) Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)
print("Train/test sizes:", X_train.shape[0], X_test.shape[0])

# 6) Define models
models = {
    "Linear Regression": LinearRegression(),
    "Random Forest": RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1),
    "Gradient Boosting": GradientBoostingRegressor(n_estimators=300, learning_rate=0.05, max_depth=4, random_state=42),
    "XGBoost": xgb.XGBRegressor(n_estimators=300, learning_rate=0.05, max_depth=4, subsample=0.8, colsample_bytree=0.8, random_state=42, n_jobs=-1)
}

# 7) Train, predict, evaluate
results = {}
predictions = {}
for name, model in models.items():
    print(f"\nTraining {name} ...")
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    predictions[name] = y_pred
    mae = mean_absolute_error(y_test, y_pred)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    r2 = r2_score(y_test, y_pred)
    results[name] = {"MAE": mae, "RMSE": rmse, "R2": r2}
    print(f"{name} -> MAE: {mae:.4f}, RMSE: {rmse:.4f}, R2: {r2:.4f}")

# 8) Show results table
res_df = pd.DataFrame(results).T
display(res_df.style.format({"MAE":"{:.4f}","RMSE":"{:.4f}","R2":"{:.4f}"}))

# 9) Plot RMSE comparison (premium-like)
plt.figure(figsize=(9,5))
order = res_df.sort_values('RMSE', ascending=False).index
sns.barplot(x=res_df.loc[order,'RMSE'].values, y=order, palette="viridis")
plt.xlabel("RMSE")
plt.title("Model RMSE Comparison")
plt.tight_layout()
plt.show()

# 10) Plot Actual vs Predicted for best model (lowest RMSE)
best = res_df['RMSE'].idxmin()
print("Best model by RMSE:", best)
y_best = predictions[best]
nplot = min(200, len(y_test))
plt.figure(figsize=(12,4))
plt.plot(np.arange(nplot), np.array(y_test)[:nplot], label="Actual", linewidth=1)
plt.plot(np.arange(nplot), np.array(y_best)[:nplot], label=f"Predicted ({best})", linewidth=1)
plt.legend()
plt.title(f"Actual vs Predicted - {best} (first {nplot} samples)")
plt.tight_layout()
plt.show()
# 11) Feature importance (for XGBoost or tree models)
if best in ["XGBoost","Random Forest","Gradient Boosting"]:
    model_for_fi = models[best]
    try:
        importances = model_for_fi.feature_importances_
        fi = pd.DataFrame({'feature': X_train.columns, 'importance': importances}).sort_values('importance', ascending=False).head(20)
        plt.figure(figsize=(8,6))
        sns.barplot(x='importance', y='feature', data=fi, palette="magma")
        plt.title(f"Top Feature Importances ({best})")
        plt.tight_layout()
        plt.show()
    except Exception as e:
        print("Feature importance error:", e)

# 12) Cross-validation for XGBoost (optional)
print("\n5-fold CV RMSE for XGBoost (optional):")
kf = KFold(n_splits=5, shuffle=True, random_state=42)
cv_mse = -cross_val_score(models['XGBoost'], X, y, scoring='neg_mean_squared_error', cv=kf, n_jobs=-1)
cv_rmse = np.sqrt(cv_mse)
print("XGBoost CV RMSE mean: {:.4f}, std: {:.4f}".format(cv_rmse.mean(), cv_rmse.std()))

# 13) Save best model and results to drive (optional)
from joblib import dump
os.makedirs("results", exist_ok=True)
dump(models[best], f"results/{best}_model.joblib")
with open("results/metrics.json", "w") as f:
    json.dump(results, f, indent=4)
print(f"\nSaved best model and metrics in ./results ({best})")
